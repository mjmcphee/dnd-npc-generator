# Ollama Configuration for AI-powered NPC Generation
# Copy this file to .env and configure your settings

# Ollama server URL (default: http://localhost:11434)
OLLAMA_HOST=http://localhost:11434

# Ollama model name to use for NPC generation
# Popular options: llama3, llama3.1, mistral, codellama, phi3
# Make sure the model is installed: ollama pull <model_name>
OLLAMA_MODEL=llama3

# Example configurations for different setups:
#
# Local Ollama (default):
# OLLAMA_HOST=http://localhost:11434
# OLLAMA_MODEL=llama3
#
# Remote Ollama server:
# OLLAMA_HOST=http://192.168.1.100:11434
# OLLAMA_MODEL=mistral
#
# Custom port:
# OLLAMA_HOST=http://localhost:8080
# OLLAMA_MODEL=llama3.1